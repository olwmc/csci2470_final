{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424744d2-24b4-4fd1-a4b1-f25cad144213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def get_acts(model, prompts):\n",
    "    # The number of layers our model has. GPT2-medium has 24\n",
    "    layers = range(model.cfg.n_layers)\n",
    "\n",
    "    # This is going to hold all of our activations. Notice the shape here: [n_prompts, n_layers, d_model]\n",
    "    data = torch.zeros((len(prompts), len(layers), model.cfg.d_model))\n",
    "\n",
    "    # For every prompt\n",
    "    for i, prompt in tqdm(enumerate(prompts)):\n",
    "        # Do a forward pass with the LLM on said prompt. This function lets us\n",
    "        # cache the activations.\n",
    "        _, activations = model.run_with_cache(prompt)\n",
    "\n",
    "        # For every layer, go through and grab the activation we want at that layer\n",
    "        # The \"[0, -1]\" there is just getting the first batch (we do one batch at a time, this\n",
    "        # could probably be improved) and then the last token at that batch (the last token\n",
    "        # in the residual stream probably (if some literature is correct) contains the \"most\n",
    "        # information\". This is the last token /in the residual stream/, not like \"dog\" in\n",
    "        # \"John has a dog\". We could experiment if this is the right place/token to try but\n",
    "        # that's for another day\n",
    "        for j in layers:\n",
    "            # Store that activation!\n",
    "            data[i, j] = activations[f'blocks.{j}.hook_resid_post'][0,-1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47c75c-fce3-4467-8136-c96360c7721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"community-datasets/generics_kb\", \"generics_kb_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23163aee-a046-485a-a3fb-d736fe886eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sentences = ds['train']['generic_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318a1e0-6a0a-420f-b23d-21fcc105471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_10k = sentences[0:10_000]\n",
    "\n",
    "acts = get_acts(gpt2_medium, first_10k)\n",
    "\n",
    "torch.save(acts, \"generics_kb_10k_10272024.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
