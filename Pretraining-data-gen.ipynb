{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "424744d2-24b4-4fd1-a4b1-f25cad144213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "def get_acts(model, prompts):\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    # The number of layers our model has. GPT2-medium has 24\n",
    "    layers = range(model.cfg.n_layers)\n",
    "\n",
    "    # This is going to hold all of our activations. Notice the shape here: [n_prompts, n_layers, d_model]\n",
    "    data = torch.zeros((len(prompts), len(layers), model.cfg.d_model))\n",
    "\n",
    "    # For every prompt\n",
    "    for i, prompt in tqdm(enumerate(prompts)):\n",
    "        # Do a forward pass with the LLM on said prompt. This function lets us\n",
    "        # cache the activations.\n",
    "        _, activations = model.run_with_cache(prompt)\n",
    "\n",
    "        # For every layer, go through and grab the activation we want at that layer\n",
    "        # The \"[0, -1]\" there is just getting the first batch (we do one batch at a time, this\n",
    "        # could probably be improved) and then the last token at that batch (the last token\n",
    "        # in the residual stream probably (if some literature is correct) contains the \"most\n",
    "        # information\". This is the last token /in the residual stream/, not like \"dog\" in\n",
    "        # \"John has a dog\". We could experiment if this is the right place/token to try but\n",
    "        # that's for another day\n",
    "        for j in layers:\n",
    "            # Store that activation!\n",
    "            data[i, j] = activations[f'blocks.{j}.hook_resid_post'][0,-1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da47c75c-fce3-4467-8136-c96360c7721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1020868"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import random\n",
    "import torch\n",
    "\n",
    "gpt2_medium = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "ds = load_dataset(\"community-datasets/generics_kb\", \"generics_kb_best\")\n",
    "sentences = ds['train']['generic_sentence']\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9749aecd-7265-489b-8f06-ba0f19571fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6318a1e0-6a0a-420f-b23d-21fcc105471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zebras are gregarious under conditions of abundant food or around water holes.', 'Optimum performance requires the maintenance of blood glucose, as well as muscle and liver glycogen.', 'Some acorn weevils have thin snouts.', 'Crocodiles prefer habitats.', 'Owls are forest creatures that hunt at night.', 'All babies thrive on the love and attention of their parents and families.', 'Arachidonic acid is found only in animal fats.', 'Biodegradable waste breaks down into methane in the landfill, if at all.', 'Orange marmalade is marmalade', 'Wood is diffuse, porous.']\n"
     ]
    }
   ],
   "source": [
    "first_50k = sentences[0:50_000]\n",
    "print(first_50k[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "132d58f6-3a64-46b0-bbef-fba6d7089593",
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = get_acts(gpt2_medium, first_50k)\n",
    "\n",
    "torch.save(acts, \"generics_kb_50k_11132024.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
