\documentclass[14pt]{article}
\usepackage[margin=0.5in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{multirow}
\pagenumbering{gobble}

% Makes stuff clickable
% \usepackage{hyperref}
% \hypersetup{
%     colorlinks,
%     citecolor=black,
%     filecolor=black,
%     linkcolor=black,
%     urlcolor=black
% }

\title{FREEZE!: Pretrained Language Models as Frozen Feature Extractors for Semantic Tasks}
\author{
    Hayden McDonald\\hayden\_mcdonald@brown.edu \and 
    Nicholas Marsano\\nicholas\_marsano@brown.edu\and 
    Oliver McLaughlin\\oliver\_mclaughlin@brown.edu
}

\begin{document}

\maketitle

% Title: Summarizes the main idea of your project.
% Who: Names and logins of all your group members.

\begin{abstract}
    We investigate whether frozen language models can be used effectively as feature extractors for semantic tasks without any fine-tuning. Our approach takes the intermediate activations of GPT2-medium and treats them as independent sources of semantic information, processing them through a combination of autoencoder pretraining and siamese networks. Despite GPT2-medium's known limitations in generating quality embeddings, we achieve \verb|76.86%| Pearson correlation on the STSB benchmark without touching the base model's weights. Interestingly, we found that widening the bottleneck of our autoencoders at later layers significantly improved performance, suggesting these layers encode richer semantic information that requires more representational capacity to capture. Our analysis of model errors revealed cases where predictions diverged notably from human ratings, raising deeper questions about the nature of semantic similarity itself. While not state-of-the-art, our results demonstrate that frozen LLM representations contain substantial semantic information that can be extracted efficiently, offering a lightweight alternative to traditional fine-tuning approaches.
\end{abstract}
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
\tableofcontents

\section{Introduction}
Our project is an investigation into a few related areas. In short, we wanted to see if we could develop an architecture that could take the intermediate activations of a pre-trained decoder-only large language model (Henceforth LLM) and use them to do text-embedding tasks. We chose semantic sentence similarity (STS) as our primary focus and motivation because of its simplicity. This task involves training a model to rate the semantic similarity of a pair of sentences. Importantly, in contrast to most other work using LLMs for text embedding, we are \textit{not} fine-tuning the model. We are simply taking a subset of the intermediate activations and treating them as a kind of \textit{"feature extracted"} (frozen) representation.

We chose this idea for several reasons.
\begin{enumerate}
    \item Our original motivational question was: \textit{"Do ‘similar’ prompts (to human readers) have ‘similar’ (under some metric) representations to a given language model?"}. By training a sentence-similarity model on LLM activations, we believed we could make progress on answering this question by \textit{learning} a transformation of the activations that correlates to semantic similarity.

    \item \textit{If} we could train a small model to interpret a given LLM's activations and produce a useful text embedding (or set of embeddings) from them, we could skip costly fine-tuning and simply pre-train and fine-tune the small "interpreter" model. Pretrained small LLMs are cheap to do forward passes on and very plentiful, so if we could \textit{just} use the intermediate activations and not have to do backward passes we could get a lot of functionality for very little compute.
    
    \item We also wanted to investigate the question: \textit{Do multiple intermediate activations perform better than just the last?} Typically when doing finetuning or transfer learning you remove the classifier end and use the last intermediate representation as your "feature extracted representation". We wanted to see if there was any performance benefit in using \textit{multiple layer's} worth of representation. This is clearly the case in the finetuning setting \cite{tang2024poolingattentioneffectivedesigns} but it's unclear if it will work here.
    
    \item From an interpretability standpoint, most distance or similarity measures (E.g cosine similarity) do not correlate strongly with \textit{human ideas} of similarity (Figure \ref{fig:raw-corr}). By training a similarity learner on LLM activations, grounded in human ideas of similarity, we hoped to produce a more interpretable measure of similarity for LLM activations.
    
    \item There just isn't that much work on this particular topic. For this task you'd typically finetune the LLM using LoRA \cite{hu2021loralowrankadaptationlarge} or simply just use an existing text embedding model. We hoped to discover interesting properties and learn a lot about what it's like to try and \textit{"disentangle"} an LLM's intermediate activations into something useful by pursuing this project. \footnote{There was only one paper we could find doing a similar setup -- extracing latents from an LLM and applying them to a task -- but it is very low quality and does not reveal many of the important details \cite{gpt2malware}. There are many papers \textit{like} this one (\cite{lu2021pretrainedtransformersuniversalcomputation} comes to mind) but almost none completely freeze the entire model and "start from scratch" as we did here.} We're not really interested in \textit{what} information is \text{where}, we just want to see \textit{if it's useable} for a simple task like STS.
    
\end{enumerate}
We chose \verb|gpt2-medium| (355M parameters) as our \textit{"base model"} for study because Oliver was familiar with its architecture and it readily fit onto all of our GPUs. We also did minor experiments on \verb|qwen2-0.5B| (391M parameters). Moreover we chose \verb|gpt2-medium| because of its very low quality text generation ability and verifiably low quality embeddings \cite{ethayarajh2019contextualcontextualizedwordrepresentations} (This paper tests the larger \verb|gpt2| base model which is more powerful. Our model is a smaller, less performance version of that). It was not at all a priori obvious to us that its representations would be fit for this task.
\section{Related Work}
% Related Work: Are you aware of any, or is there any prior work that you drew on to do your project?
%     Please read and briefly summarize (no more than one paragraph) at least one paper/article/blog relevant to your project.
Reusing pretrained LLMs for semantic tasks like STS is nothing new. There is extensive literature about finetuning and retraining
these models for a variety of new tasks \cite{reimers2019sentencebertsentenceembeddingsusing}, \cite{tang2024poolingattentioneffectivedesigns}. Moreover using decoder-only models for embedding tasks is common, with works like
SGPT \cite{muennighoff2022sgptgptsentenceembeddings}, LLM2VEC \cite{LLM2Vec}, LLMEmbed \cite{LLMEmbed} etc. exploring this idea as well as "meta-techniques" for improving these embeddings like Mitchell et. al's paper on how repeating a given piece of text within a prompt improves downstream benchmark performance after collecting a truncated subset of the generated latents \cite{RepImprovesLLM}. It is also well known that GPT-2 produces incredibly low quality embeddings \cite{ethayarajh2019contextualcontextualizedwordrepresentations} and even basic exploration shows that the smaller \verb|gpt2-medium| produces very low quality text generation.
Our work also takes inspiration from other parameter-efficient finetuning methods

% Our work takes inspiration from adapter models but we leverage a key differnce -- Our approach takes advantage of the fact that different layers encode meaningfully different information and we treat them as independent sources of information. This is in contrast to adapter models which simply modify the existing network but keep the same flow of information. Moreover, adapter models are constrained by needing to create a single representation that conforms to the existing model's "hardware", so to speak. Our approach is completely free to develop whatever representation is needed to extract relevant information.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.5\linewidth]{raw_cosine_sim.png}
    \caption{\textbf{The problem we are ultimately trying to solve}. Layerwise average correlation between gpt1-medium's embeddings and their cosine similarity vs. human similarity score for the STS training pairs}
    \label{fig:raw-corr}
\end{figure}

\subsection{Problem Setup} \label{Setup}
Figure \ref{fig:raw-corr} was our "starting point". This figure depicts the correlation between two important quantities for our investigation:
\begin{enumerate}
    \item The cosine similarity of two sentences' latent representations (See \ref{latent} for more information on this representation)
    \item A human rated similarity score for those same two sentences (See \ref{STS-section})
\end{enumerate}
For each layer we computed the mean correlation between these two quantities for more than five thousand sentence pairs. This is essentially the problem we are trying to solve -- \textit{How can we transform these latent representations into something useable for semantic text similarity?}

In short, we're trying to solve the problem of "low quality" embeddings in small LLMs like \verb|gpt2-medium| by finding an architecture which can leverage the information contained within its latents. Our target task for this experiment is semantic text similarity and we will measure success through our performance on the STSB \cite{STS} benchmark. To do this we trained a similarity learner on the STS dataset with some added unsupervised pretraining. We believe that if we can take these latent representations generated by \verb|gpt2-medium| and train a model to effectively use them that this would be evidence of these embeddings being much higher quality than originally expected in terms of their information content.


\section{Data}
We have three major datasets that we used for both pretraining and supervised STS training.
\begin{enumerate}
    \item STSB \cite{STS} -- A standard sentence similarity benchmark with associated training and test sets.
    \item GenericsKB \cite{bhakthavatsalam2020genericskbknowledgebasegeneric} -- A 'generic sentence' dataset of which we used twenty-thousand examples.
    \item SNLI \cite{snli} -- An inference relation dataset. Used in a pretraining experiment and in Sentence-BERT \cite{reimers2019sentencebertsentenceembeddingsusing}
\end{enumerate}

\subsection{Semantic Text Similarity Benchmark (STSB)} \label{STS-section}
The main task we targeted was STSB \cite{STS}, which is a benchmark for semantic text similarity. It's comprised of pairs of sentences and a human-rated "similarity score". For example:
$$
(\text{s1='A plane is taking off.'}, \text{s2='An air plane is taking off.'}, \text{sim}=1.0)
$$
The performance metrics for STS are typically Spearman's rank correlation coefficient and normal Pearson correlation between the predicted similarities of pairs of sentences and the human rated similarities. State of the art for this task is around \verb|90 - 92%| in both spearman and pearson according to the MTEB leaderboard \cite{muennighoff2022mteb}.

\subsection{Data Collection and Pipeline} \label{latent}
Our data pipeline is fairly simple. We're treating \verb|gpt2-medium| as a feature extractor, so for each piece of text for any task, we simply pass that text through the model and extract a latent representation from the intermediate activations at each layer. We chose to take the \textit{last token at each layer, after the residual connection} (Henceforth called a \textit{latent}). This strategy is not original and is fairly standard in the \textit{fine tuning} literature \cite{LLMEmbed}. We expected it to do fine for our task given that \verb|gpt2-medium| uses causal attention (and thus the last token has "seen" all of the previous tokens), even though we're not fine tuning. This gives us, for each sentence, twenty-four \verb|1024| dimensional vectors (\verb|n_layers x d_model|). One of the main challenges we wanted to overcome by taking this project on was finding a sensible way to \textit{actually use} such a high volume and dimensionality of data.
% Data: What data are you using (if any)?

%     If you’re using a standard dataset (e.g. MNIST), you can just mention that briefly. Otherwise, say something more about where your data come from (especially if there’s anything interesting about how you will gather it).
%     How big is it? What kind of preprocessing was required?

\section{Methodology}
% Methodology: What is the architecture of your model?
%     How are you training the model?
%     Justify your design. Also note some things you tried that may or may not have worked.
The key insight behind our approach is that different layers in transformer models encode distinct types of information, with each layer potentially discarding or transforming information from previous layers as it builds towards its final objective. Rather than assuming later layers contain all necessary information, we hypothesized that earlier layers might preserve valuable semantic features that get filtered out during the progression through the model. While this is similar in concept to adapter models (CITE ME), our approach has a key advantage: since we're not constrained by the need to maintain the original model's information flow, we can treat each layer's representational space as fully independent. This allows us to potentially capture and utilize semantic features that might otherwise be lost or transformed as information propagates through the model's layers.

\subsection{Training and Loss function} \label{Loss} \label{Training}
Our general training scheme was the following:
\begin{enumerate}
    \item Pass whatever example sentence for the current task through \verb|gpt2-medium|
    \item Extract the latents (Described in Section \ref{latent}) from each layer. We take these latents and completely decouple them from the base model. They are effectively just a set of vector representations of the input. No gradient is passed back through \verb|gpt2-medium|
    \item Pass those latents into our model.
\end{enumerate}
To train our model to do STS we pass in both pairs of sentences to \verb|gpt2-medium| as described above, pass those latent vectors into our model and extract a scalar similarity value. We then do supervised training where the ground truth is the human-rated similarity value of the two sentences as given by the STSB dataset.

\subsubsection{Loss function}
Our loss function evolved throughout our experiments. Initially, we used mean squared-error (MSE) loss between the predicted similarities for a sentence pair and the human-rated similarities from the STSB dataset. For our final model, however, our loss function for supervised STS training departs from traditional MSE in that we only minimize the \text{variance of the residuals}, instead of both variance \text{and} bias which are traditionally minimized when using MSE. While our model's performance is evaluated using both Pearson and Spearman correlation (which measure linear and monotonic relationships respectively), we found that simply minimizing residual variance without enforcing zero bias led to strong performance on both metrics. Since our model uses a sigmoid activation at the output layer, both our predictions and ground truth are bounded in [0,1], which naturally constrains the residuals to [-1,1]. This architectural choice means the residuals cannot have arbitrary bias even though we don't explicitly optimize for it. Given these bounds, we can focus our training objective purely on minimizing variance to promote consistent relationships between predictions and ground truth. Empirically we found that our residuals centered themselves near zero even though we didn't enforce this directly. We found that taking the log of this value improved training stability and overall performance. The log transformation provides more balanced gradients across different scales of variance, preventing optimization instability when variances are very large while maintaining sensitivity to improvements when variance is already small. Our final loss function is then as follows:
$$
\mathcal{L}(\theta) = \log(\text{Var}(y_\theta - y))
$$
Where $y_\theta$ is the predicted similarity scores and $y$ is the ground truth. We only saw meaningful improvement with this new loss metric on the final architecture, so our experimental results reported for all but the last model used MSE.

\subsection{Architecture}
Our architecture was decided through a series of experiments and trial and error. We tried to only make architectural decisions which were, in some way, \textit{principled} -- I.e. we had a decent reason for making that particular change.

\subsubsection{First iteration}
Our first architecture was incredibly simple -- Just take the layerwise cosine similarities described in Section \ref {Setup} and pass them through an MLP to do basic weighting and statistical correlation. We felt this was reasonable because although the individual correlations were quite weak, it wasn't clear to us that each layer was correlating \textit{to the same content}. In the same way that many weak models can be combined to create a strong one, we figured that a simple MLP would be able to decipher some of the more complicated relationships, even just through the layerwise similarities. This model was quite simple, just a one layer mlp that took the twenty-four layerwise cosine similarities from \verb|gpt2-medium|'s respective latents and outputted a sigmoided single scalar. This was trained using the scheme described in Section \ref{Training}. This model significantly improved upon baseline, giving us a test set pearson correlation of \verb|39.2%| and spearman of \verb|42.06%| ($p<< 0.001$)

\subsubsection{Layerwise Siamese Networks}
After the previous architectural experiment, we figured that we were probably bottlenecked by the representation of our base model, necessitating a transformation. Inspired by prior work on Siamese networks for semantic tasks \cite{reimers2019sentencebertsentenceembeddingsusing}, we decided to modify our architecture by applying siamese networks to each layer, computing the cosine similarities of \text{those} representations, and then finally passing those cosine similarities into a weighting/pattern matching MLP as described above. The intuition here is that each layer probably contains meaningfully different and useful information, as evidence by weighted cosine doing better than any particular layer. So, if we learn a nonlinear transformation of each layer's particular space through supervised STS training, we might be able to extract a better representation for this task and have overall better downstream performance.

This architecture achieved a \text{significant} performance increase, moving our pearson and spearman on the test set to \verb|71.63%| and \verb|69.72| ($p << 0.001$) respectively.

\subsubsection{Autoencoder pretraining and the final model}
We then realized that although this architecture was traning quite well, it was likely the case that inputs which were significantly different from those in the input distribution would likely not be well represented by our siamese networks. This then inspired us to change our training strategy. Our current strategy \text{coupled} our representation learning and our supervised STS learning. That is, the model had to both learn a good representation of the input whilst also learning how to orient examples spatially to reflect semantic similarity. We then decided to break this up by first training a set of layerwise autoencoders over twenty-thousand "generic" sentences from the GenericsKB \cite{bhakthavatsalam2020genericskbknowledgebasegeneric} dataset. These senteces were similar in length and complexity to the STS training set that we felt it was an appropriate 'pretraining' dataset. We trained the autoencoders to reconstruct each layer's latent for each sentence (I.e. twenty-four autoencoders, each trying to reconstruct their respective layer's latent). We then did this pretraining and took the encoder part of each autoencoder and treated it as our new siamese network. Each autoencoder had a bottleneck of $256$ hidden units.

Immediately, our results were \textit{indistinguishable} from the results of the previous section. We then produced Figure \ref{fig:autoencoder-loss} and realized that the layerwise losses for each autoencoder increased exponentially as the layers went on. This inspired us to, among many other things that did not work, widen the bottleneck of the last twelve autoencoders from $256$ to $512$. This then finally showed a performance increase to \verb|75.36%| pearson and \verb|73.21%| spearman. Our final change was to incorperate the loss function switch described in Section \ref{Loss}, which brought us to a final pearson and spearman correlation of:
\begin{align*}
    &\text{Pearson} = 76.86\%\\
    &\text{Spearman} = 75.03\%\;,\; p << 0.001
\end{align*}
Moreover, this architecture actually ended up with more than $600,000$ fewer parameters when compared to our previous best model. Importantly, the \textit{only} way we saw an increase in downstream performance, after many hyperparameter experiments and automated tuning, was to \textit{widen the bottleneck at the later layers}. This heavily suggests that the information present at later latent vectors is not only much richer (And thus requiring many more bases to represent) but also incredibly important for semantic tasks. Critically, \textit{deepening} the autoencoders \textit{did not improve downstream performance}, even though doing so added millions of paramters. Only widening the bottleneck improved performance.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{encoder_loss.png}
    \caption{Autoencoder loss by layer. $\text{d}_\text{bottleneck} = 256$}
    \label{fig:autoencoder-loss}
\end{figure}

\subsection{Experiments that failed}


% Our loss function has evolved significantly since the beginning. We plotted the distribution of our residuals and noticed
% the spread could be tightened. Since this is a correlative task, we don't care about bias, just variance. So instead of doing MSE
% we just optimize to minimize the variance of the residuals directly
% $$
% \mathcal{L}(\theta) = \log(\text{Var}(y - \hat{y}))
% $$
% Where the added log improved stability

\section{Results}
\subsection{Requirements for success}
Our main notion of success was performance on the STSB \cite{STS} benchmark as described in Section \ref{STS-section}. Pearson/spearman correlation are both reasonable measures of success on this task because they measure how consistent your model is with human-rated similarity notions, regardless of scale or center. State of the art on this task is \verb|90-92%| for both Spearman and Pearson correlation as of the writing of this document \cite{muennighoff2022mteb}. Our goal was to achieve at least \verb|60%| pearson correlation. We really did not have strong reasoning for why this might be a reasonable goal but preliminary results showed that at least \verb|40%| was possible so we decided that \verb|60%| was a reasonable stretch goal

\subsection{Final Results}
We achieved a Pearson correlation on STSB's test set of \verb|76.86%| and a Spearman correlation of \verb|75.3%| with a p-value of significantly less than \verb|0.0001|.
% Results: What constitutes “success?”
%     What experiments did you run?
%     For most of our assignments, we have looked at the accuracy of the model. Does the notion of “accuracy” apply for your project, or is some other metric more appropriate?
%     Explain how you assess your model's performance and justify why it is a reasonable measurement.

\section{Ethics}
% Ethics: Choose 2 of the following bullet points to discuss; not all questions will be relevant to all projects so try to pick questions where there’s interesting engagement with your project. (Remember that there’s not necessarily an ethical/unethical binary; rather, we want to encourage you to think critically about your problem setup.)
%     What broader societal issues are relevant to your chosen problem space?
%     Why is Deep Learning a good approach to this problem?
%     What is your dataset? Are there any concerns about how it was collected, or labeled? Is it representative? What kind of underlying historical or societal biases might it contain?
%     Who are the major “stakeholders” in this problem, and what are the consequences of mistakes made by your algorithm?
%     How are you planning to quantify or measure error or success? What implications does your quantification have?
%     Add your own: if there is an issue about your algorithm you would like to discuss or explain further, feel free to do so.
There are two main societal impacts of this project. First, it addresses the relatively high barrier of entry for STS tasks by showing promise in lightweight methods that bypass the need for resource-intensive fine-tuning. This bypass offers a glimpse into the democratization of advanced NLP capabilities for researchers or organizations with limited computational resources. As more LLMs become open source, the barrier of entry to leverage them in practical applications can be lowered and allow for more innovation in smaller-scale environments. Second, the project addresses the advancement of model interpretability to increase trust in deployed models. By training an architecture to derive meaningful embeddings from intermediate activations, it contributes to the broader understanding of how LLMs encode semantic information. By helping to bridge the gap between machine representation and human comprehension, this project helps to further AI transparency as it plays a more dominant role in everyday life. 

\section{Discussion}

\subsection{What did the model learn?}
An interesting question to ask here is \textit{What kind of semantic similarity did our model learn?} Although STSB has a particular idea of semantic similarity (Namely, that two sentences 'mean' the same thing), true \textit{"semantic similarity"} is not limited to just identical ideas being expressed in different ways. We could easily define semantic similarity to be in regards to the abstract character of the sentences, or of the \textit{ideas} they are conveying. For example, STSB labels the \textit{"semantic similarity"} of the following sentences as \verb|20%|:
\begin{verbatim}
    'Hawaii passes gay marriage bill', 'US Senate passes gay workers bill',
\end{verbatim}
Our model labels them at \verb|75.98%|. Which is \textit{"more"} correct? Although the literal content of the two sentences is different, they are quite similar in what they are conveying -- Bills being passed regarding LGBT civil rights. There are numerous examples exactly like this one. One weaker example is:
\begin{verbatim}
    'You will want to clean the area first.', 'You will also want to remove the seeds.',
\end{verbatim}
Which STSB has rated at a similarity of \verb|0%|. Our model is less conservative in its estimate at \verb|56.01%|. Neither interpretation is wrong and clearly our model makes obvious and unavoidable errors (See Section \ref{error analysis} for more details) but in many contexts -- such as following steps in a recipe -- these sentences could be functionally identical, both indicating preparatory steps in a sequence.

This raises deeper questions about the nature of semantic similarity itself. Two humans, given different contexts or purposes (e.g., legal analysis vs. historical study), might rate these sentences quite differently. Our model's architecture, processing information across multiple transformer layers, may naturally capture these various levels of similarity -- from surface-level linguistics to broader themes. This observation, supported by our finding that wider bottlenecks in later layers improved performance, suggests that our model's "errors" might actually be revealing limitations in how we define and measure semantic similarity, rather than just limitations in the model itself. Of course this does not distract from the fact that our model is nowhere near state of the art, but it is a worthwhile question to ask either way. Clearly our training produced \textit{a particular interpretation of semantic similarity}, but which was it? Excluding clear errors, of course. We offer more insight into this in Section \ref{error analysis}.

\subsection{What did we actually create?}
Although our original motivation was in the vein of interpretability, our approach combines several potentially useful components:
\begin{itemize}
    \item A distinct, layerwise-independent parameter-efficient approach to adapting existing LLMs to sentence similarity without fine-tuning
    \item A similarity measure for \verb|gpt2-medium|'s activations that aligns with human judgments of semantic similarity, potentially offering more interpretable insights into the model's representations
    \item A representation learning scheme for LLM activations that preserves the base model's weights and information flow while enabling task-specific adaptation
\end{itemize}
While our performance does not match state-of-the-art models, these techniques might prove useful for understanding and utilizing frozen LLM representations in other contexts. In future work we aim to see how far these individual components can be improved.

\subsection{\textit{"What about a bigger base model?"} and the Representational Ceiling}
We ran many, many hyperparameter experiments on the autoencoders and did not find any benefit (despite significant effort) in downstream performance beyond the aforementioned widening of the bottleneck at later layers. This suggests an important limitation of our setup -- \textit{The representational capability of our base model}. In the finetuning scenario the entire "representational machinery" of the base model is appropriated for the new task. Millions of parameters are modified and retuned to throw away useless information (wrt the task at hand) and prioritize information that improves performance on the new selected metric. In our situation, we are \textit{"stuck with"} whatever representation the original training for \verb|gpt2-medium| produced. Critically, this representation is not tuned for semantic similarity tasks, but for next token prediction. It is likely that these two objectives require substantially different representations (and thus contextual information) of the input to perform well at.

A natural extension to our work would be to try a larger and more capable base model. Our initial experiments used \verb|qwen2-0.5B|, which performed significantly better than \verb|gpt2-medium| on both Pearson and Spearman correlation (\verb|+~2%| on both). However, this gap disappeared once we made the changes to the loss function described in Section \ref{Loss}. After rerunning the experiments with both base models and the new loss function, we found no meaningful difference between the two despite \verb|qwen2-0.5B|'s additional parameters. This likely suggests a limitation in our experimental setup for qwen2, given that we simply substituted the base model in our original architecture and only adjusted the input dimension of the autoencoders to match. A more thorough investigation of larger models would require architectural changes tailored to their specific parameter counts. We just wanted to assure that our approach was not uniquely suitable for \verb|gpt2-medium|.
\section{Reflection}
% Reflection: please address the following questions (along with other thoughts you have about the project, if any):
%     How do you feel your project ultimately turned out? How did you do relative to your base/target/stretch goals?
%     Did your model work out the way you expected it to?
%     How did your approach change over time? What kind of pivots did you make, if any? Would you have done differently if you could do your project over again?
%     What do you think you can further improve on if you had more time?
%     What are your biggest takeaways from this project/what did you learn?

One big takeaway from this project is that as gigantic open source LLMs become more pevalent, it might be possible to chop off most of their layers and train little "interpreter" modules as we did here to leverage the incredibly rich and complex learned represenations at a fraction of the cost and computation time.

\section{Division of Labor}
% Division of labor: Briefly outline who will be responsible for which part(s) of the project.
\begin{itemize}
    \item \textit{Hayden.}
    \item \textit{Nick.} 
    \item \textit{Oliver.} 
\end{itemize}

\newpage
\section{Appendix: Qualitative Analysis of Model Errors} \label{error analysis}
To better understand our model's limitations, we sorted all sentence pairs in the STSB test set by the absolute difference between predicted and true similarity scores. Below are the five pairs with the largest errors, representing our model's most significant failures.

\begin{verbatim}
"Not 'hiding the ball' on Russia: Obama",
"Obama says he's not 'hiding the ball' on Russia",
True similarity: 0.8800
Predicted: 0.2084

"Taiwan's economy grows 2.27% in April-June quarter",
"China's economic growth rebounds to 7.8% in latest quarter",
True similarity 0.2000
0.8955

'A man is laughing with a woman',
'A man and a woman laughing.',
True similarity: 0.9600
Predicted: 0.2572

'Supreme Court to Hear Voting Rights Act Case',
'Supreme Court to hear corporate human rights case',
True similarity: 0.2800
Predicted: 0.9893

"Hassan Rouhani wins Iran's presidential election",
'Maduro wins Venezuelan presidential vote',
True similarity: 0.0400
Predicted: 0.8193
\end{verbatim}
These are just the worst performing sentence pairs but they show a trend seen by browsing the top fifty or so. Clearly our model has a different idea of similarity than STSB does, but that model isn't always completely off base. Consider the last sentence pair in this set. Both sentences describe a particular figure winning a particular election. Obviously, semantically, these two sentences are nothing alike, they are describing two completely different figures winning two completely different elections, but they are "semantically similar" in a more abstract sense. Similarly with the Taiwan/China pair and the supreme court pair. That being said, the model clearly also just fails to see the similarity in some pairs, as seen in the man and woman laughing pair. Interestingly, the model also produces the following prediction:
\begin{verbatim}
'A brown dog is jumping.',
'A brown dog is jumping',
True similarity: 1
Predicted: 0.4735
\end{verbatim}
Which shows that the model has \textit{not learned to recognize completely identical sentences}. This might imply that our training set could use more identical pairs or that our model needs to be restructured in such a way that the ones vector (\verb|(1, 1, ..., 1)|) produces a \verb|1| at the end of the MLP. This is because our layerwise siamese networks use cosine similarity, so the output at each layer for two identical prompts is always the ones vector. In any case, this is a significant limitation of our architecture. Moreover many of these examples show how our model confuses surface level syntactic similarity with deeper semantic similarity.

\newpage
\section{Appendix: \textit{STSB} dataset leakage} 
\newpage
\bibliography{mybib}{}
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\end{document}
