{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea55baec-0cab-4dbc-8958-7c43c287f470",
   "metadata": {},
   "source": [
    "Hypothesis: We might be representation bottlenecked and that might be contributing to our less than ideal performance. What if we start by pretraining our network to do autoencoding, chop off the decoder, and then do our metric learning on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4219dad1-3960-4a7c-a817-8cbae0a95aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df28cd2-1819-4efa-a7dc-7170952551e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f4a215-2581-4e52-b972-172453ed57a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bufo/current/research/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-medium\")\n",
    "ds = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\")\n",
    "ds_train = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adfcc36-85ed-4586-9280-8812f5b5eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts(model, prompts):\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    # The number of layers our model has. GPT2-medium has 24\n",
    "    layers = range(model.cfg.n_layers)\n",
    "\n",
    "    # This is going to hold all of our activations. Notice the shape here: [n_prompts, n_layers, d_model]\n",
    "    data = torch.zeros((len(prompts), len(layers), model.cfg.d_model))\n",
    "\n",
    "    # For every prompt\n",
    "    for i, prompt in tqdm(enumerate(prompts)):\n",
    "        # Do a forward pass with the LLM on said prompt. This function lets us\n",
    "        # cache the activations.\n",
    "        _, activations = model.run_with_cache(prompt)\n",
    "\n",
    "        # For every layer, go through and grab the activation we want at that layer\n",
    "        # The \"[0, -1]\" there is just getting the first batch (we do one batch at a time, this\n",
    "        # could probably be improved) and then the last token at that batch (the last token\n",
    "        # in the residual stream probably (if some literature is correct) contains the \"most\n",
    "        # information\". This is the last token /in the residual stream/, not like \"dog\" in\n",
    "        # \"John has a dog\". We could experiment if this is the right place/token to try but\n",
    "        # that's for another day\n",
    "        for j in layers:\n",
    "            # Store that activation!\n",
    "            data[i, j] = activations[f'blocks.{j}.hook_resid_post'][0,-1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad588d5-db70-485d-bb60-66bb0925019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just take the first 10k sentence pairs\n",
    "\n",
    "idxs = random.sample(range(len(ds_train)), 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00842572-4bd8-4d5c-a7d9-09f79ecf3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ds_train.select(idxs)\n",
    "premises = subset['premise']\n",
    "hypotheses = subset['hypothesis']\n",
    "labels = subset['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c1c807-bec1-4a51-981e-dc210e0cc77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [09:06, 18.29it/s]\n",
      "10000it [09:07, 18.27it/s]\n"
     ]
    }
   ],
   "source": [
    "premise_acts = get_acts(model, premises)\n",
    "hypothesis_acts = get_acts(model, hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1e373ee-6306-4045-be86-aba9ac80f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(premise_acts, \"premise_acts.pt\")\n",
    "torch.save(hypothesis_acts, \"hypothesis_acts.pt\")\n",
    "torch.save(labels, \"labels.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
