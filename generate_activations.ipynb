{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9556a9fa-76c9-422c-b901-549a2decf905",
   "metadata": {},
   "source": [
    "This notebook demonstrates generating the activations from gpt2-medium for each pair of prompts in the [STS benchmark dataset](https://paperswithcode.com/dataset/sts-benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0345e76-6c57-402e-8e34-08147bdb8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from datasets import load_dataset # Datasets is hugging face's way of distributing their data\n",
    "from transformer_lens import HookedTransformer # This library allows us to grab the activations from pretrained LLM's\n",
    "import torch # Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd02a843-ad17-4f06-be17-e3bac22beba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acts(model, prompts):\n",
    "    # The number of layers our model has. GPT2-medium has 24\n",
    "    layers = range(model.cfg.n_layers)\n",
    "\n",
    "    # This is going to hold all of our activations. Notice the shape here: [n_prompts, n_layers, d_model]\n",
    "    data = torch.zeros((len(prompts), len(layers), model.cfg.d_model))\n",
    "\n",
    "    # For every prompt\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        # Do a forward pass with the LLM on said prompt. This function lets us\n",
    "        # cache the activations.\n",
    "        _, activations = model.run_with_cache(prompt)\n",
    "\n",
    "        # For every layer, go through and grab the activation we want at that layer\n",
    "        # The \"[0, -1]\" there is just getting the first batch (we do one batch at a time, this\n",
    "        # could probably be improved) and then the last token at that batch (the last token\n",
    "        # in the residual stream probably (if some literature is correct) contains the \"most\n",
    "        # information\". This is the last token /in the residual stream/, not like \"dog\" in\n",
    "        # \"John has a dog\". We could experiment if this is the right place/token to try but\n",
    "        # that's for another day\n",
    "        for j in layers:\n",
    "            # Store that activation!\n",
    "            data[i, j] = activations[f'blocks.{j}.hook_resid_post'][0,-1]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e16cf8-1e93-48da-9167-131d965def68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bufo/current/research/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Here's an example. Let's load up gpt2 medium\n",
    "gpt2_medium = HookedTransformer.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e58a753d-97f6-4047-abfa-7a8ee437a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then grab the activations for a few simple prompts. Let's just verify the shape is right\n",
    "acts = get_acts(gpt2_medium, [\"John is a great cook\", \"I don't know where my phone is\"])\n",
    "\n",
    "# Make sure our shape is right\n",
    "assert(list(acts.shape) == [2,24,1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60e777-bdee-450c-ad10-bbbc962188a9",
   "metadata": {},
   "source": [
    "So we can grab the activations for a set of prompts. Let's do it for the train and test sets of the STS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69868ac3-c90e-494b-9e82-e7df38b71abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_activations_for_model():\n",
    "    test_set  = load_dataset(\"sentence-transformers/stsb\", split=\"test\")\n",
    "    train_set = load_dataset(\"sentence-transformers/stsb\", split=\"train\")\n",
    "\n",
    "    # Grab the activations for all of the test prompts. Both the first and the second sentence.\n",
    "    test_acts_1 = get_acts(gpt2_medium, test_set['sentence1'])\n",
    "    test_acts_2 = get_acts(gpt2_medium, test_set['sentence2'])\n",
    "\n",
    "    # Assert their shapes are right and that the first index is different (To make sure we actually\n",
    "    # computed two different sets of activations)\n",
    "    assert(list(test_acts_1.shape) == [1379, 24, 1024])\n",
    "    assert(test_acts_1.shape == test_acts_2.shape)\n",
    "    assert(not torch.equal(test_acts_1[0], test_acts_2[0]))\n",
    "\n",
    "    # Now for the train set\n",
    "    train_acts_1 = get_acts(gpt2_medium, train_set['sentence1'])\n",
    "    train_acts_2 = get_acts(gpt2_medium, train_set['sentence2'])\n",
    "\n",
    "    # Again some nice asserts\n",
    "    assert(list(train_acts_1.shape) == [5749, 24, 1024])\n",
    "    assert(train_acts_1.shape == train_acts_2.shape)\n",
    "    assert(not torch.equal(train_acts_1[0], train_acts_2[0]))\n",
    "\n",
    "    # Now we save everything\n",
    "    torch.save(test_acts_1, \"gpt2_medium_test_acts_1.npy\")\n",
    "    torch.save(test_acts_2, \"gpt2_medium_test_acts_2.npy\")\n",
    "\n",
    "    torch.save(train_acts_1, \"gpt2_medium_train_acts_1.npy\")\n",
    "    torch.save(train_acts_2, \"gpt2_medium_train_acts_2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e9ae0a5-3b5c-47c7-8a7e-54e0c7c6464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_activations_for_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
